{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOjC74bto/2sx0HO0M9XB3y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sunday-Okey/NLP_Project_Machine_Translation/blob/master/NLP_Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import the libraries\n",
        "import collections\n",
        "import helper\n",
        "import numpy as np\n",
        "import project_tests as tests\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model\n",
        "from keras.layers import LSTM, GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional, Dropout\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import sparse_categorical_crossentropy\n",
        "import os\n",
        "import string\n",
        "import requests"
      ],
      "metadata": {
        "id": "CaUagM43ir9X"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the data"
      ],
      "metadata": {
        "id": "bzkxYJgXFQz0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [Data Source](https://www.statmt.org/europarl/)"
      ],
      "metadata": {
        "id": "Eos670ehOK0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "folder_name = \"data\"\n",
        "url = \"https://www.statmt.org/europarl/v7/fr-en.tgz\"\n",
        "\n",
        "\n",
        "# Create the folder if it doesn't already exist\n",
        "if not os.path.exists(folder_name):\n",
        "    os.makedirs(folder_name)\n",
        "\n",
        "# Download the file and save it in the folder\n",
        "filename = os.path.join(folder_name, \"fr-en.tgz\")\n",
        "response = requests.get(url, stream=True)\n",
        "with open(filename, \"wb\") as f:\n",
        "    for chunk in response.iter_content(chunk_size=1024):\n",
        "        f.write(chunk)\n",
        "\n",
        "print(\"File saved in folder:\", folder_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QTvXv71LenR",
        "outputId": "544badee-359c-435a-b096-9c04d120411e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File saved in folder: data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbsX1RQEEhBn",
        "outputId": "d9966e33-b3a8-4926-9ec0-a1553fdbb4df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading helper.py\n",
            "Downloading project_tests.py\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/Sunday-Okey/NLP_Project_Machine_Translation/master/\"\n",
        "for filename in (\"helper.py\", \"project_tests.py\"):\n",
        "    print(\"Downloading\", filename)\n",
        "    url = DOWNLOAD_ROOT + filename\n",
        "    urllib.request.urlretrieve(url, filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract the file"
      ],
      "metadata": {
        "id": "JRJAGqIdPZik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "\n",
        "filename = \"data/fr-en.tgz\"\n",
        "folder = \"data\"\n",
        "\n",
        "# Extract the files from the archive\n",
        "with tarfile.open(filename, \"r:gz\") as tar:\n",
        "    tar.extractall(folder)\n",
        "\n",
        "print(\"Files extracted into the folder:\", folder)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOwIMMfiPYSO",
        "outputId": "fffbeb8b-a4be-4b52-a233-94a63a74f44a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files extracted into the folder: data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Get the current working directory\n",
        "current_dir = os.getcwd()\n",
        "\n",
        "# Specify the old file names and new names\n",
        "old_names = [\"europarl-v7.fr-en.en\", \"europarl-v7.fr-en.fr\"]\n",
        "new_names = [\"small_vocab_en\", \"small_vocab_fr\"]\n",
        "\n",
        "# Iterate over the old and new names and rename the files\n",
        "for i in range(len(old_names)):\n",
        "    old_path = os.path.join(current_dir, \"data\", old_names[i])\n",
        "    new_path = os.path.join(current_dir, \"data\", new_names[i])\n",
        "    os.rename(old_path, new_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "btqDkmLlPuz8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets read in the data"
      ],
      "metadata": {
        "id": "NRZDcc7-TZ2_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(path):\n",
        "    \"\"\"\n",
        "    Load dataset\n",
        "    \"\"\"\n",
        "    input_file = os.path.join(path)\n",
        "    with open(input_file, \"r\", encoding='utf-8') as f:\n",
        "        data = f.read()\n",
        "\n",
        "    return data.split('\\n')"
      ],
      "metadata": {
        "id": "kxaTF39FRSxp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load English data\n",
        "english_sentences = load_data('data/small_vocab_en')\n",
        "# Load French data\n",
        "french_sentences = load_data('data/small_vocab_fr')"
      ],
      "metadata": {
        "id": "9gDRYaRjUHiL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(english_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2rAfj3HUS59",
        "outputId": "23f89000-ba62-4d04-fc5f-c855087995a8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2007724"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(french_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrDK8mw7UYio",
        "outputId": "ec2e31e4-566a-45eb-e8e6-2fbbb57d1ce1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2007724"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6L7QexUmlhn5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## File\n",
        "Each line in small_vocab_en contains an English sentence with the respective translation in each line of small_vocab_fr. View the first two lines from each file"
      ],
      "metadata": {
        "id": "ySHKxbcmaYLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for sample in range(3):\n",
        "    print(f'small_vocab_en Line {sample + 1}:  {english_sentences[sample]}')\n",
        "    print(f'small_vocab_fr Line {sample + 1}:  {french_sentences[sample]}')\n",
        "    print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFDSfWtYYh5y",
        "outputId": "6bf749b5-dc20-4ba8-ebc9-e581bd683563"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "small_vocab_en Line 1:  Resumption of the session\n",
            "small_vocab_fr Line 1:  Reprise de la session\n",
            "\n",
            "\n",
            "small_vocab_en Line 2:  I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\n",
            "small_vocab_fr Line 2:  Je déclare reprise la session du Parlement européen qui avait été interrompue le vendredi 17 décembre dernier et je vous renouvelle tous mes vux en espérant que vous avez passé de bonnes vacances.\n",
            "\n",
            "\n",
            "small_vocab_en Line 3:  Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\n",
            "small_vocab_fr Line 3:  Comme vous avez pu le constater, le grand \"bogue de l'an 2000\" ne s'est pas produit. En revanche, les citoyens d'un certain nombre de nos pays ont été victimes de catastrophes naturelles qui ont vraiment été terribles.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vocabulary\n",
        "\n",
        "The complexity of the problem is determined by the complexity of the vocabulary. A more complex vocabulary is a more complex problem. Let's look at the complexity of the dataset we'll be working with."
      ],
      "metadata": {
        "id": "09sT042EiasI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()])\n",
        "french_words_counter = collections.Counter([word for sentence in french_sentences for word in sentence.split()])\n",
        "\n",
        "print('{} English words.'.format(len([word for sentence in english_sentences for word in sentence.split()])))\n",
        "print('{} unique English words.'.format(len(english_words_counter)))\n",
        "print('10 Most common words in the English dataset:')\n",
        "print('\"' + '\" \"'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '\"')\n",
        "print()\n",
        "print('{} French words.'.format(len([word for sentence in french_sentences for word in sentence.split()])))\n",
        "print('{} unique French words.'.format(len(french_words_counter)))\n",
        "print('10 Most common words in the French dataset:')\n",
        "print('\"' + '\" \"'.join(list(zip(*french_words_counter.most_common(10)))[0]) + '\"')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25_Obq_nZ4eh",
        "outputId": "97d5c432-8365-4499-b974-0e38cb681b32"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50263459 English words.\n",
            "301951 unique English words.\n",
            "10 Most common words in the English dataset:\n",
            "\"the\" \"of\" \"to\" \"and\" \"in\" \"that\" \"a\" \"is\" \"for\" \"I\"\n",
            "\n",
            "52562231 French words.\n",
            "395651 unique French words.\n",
            "10 Most common words in the French dataset:\n",
            "\"de\" \"la\" \"et\" \"le\" \"à\" \"les\" \"des\" \"que\" \"en\" \"du\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(x):\n",
        "    \"\"\"\n",
        "    Tokenize x\n",
        "    :param x: List of sentences/strings to be tokenized\n",
        "    :return: Tuple of (tokenized x data, tokenizer used to tokenize x)\n",
        "    \"\"\"\n",
        "    x = [sentence.lower().translate(str.maketrans('', '', string.punctuation)) for sentence in x]\n",
        "    t = Tokenizer()\n",
        "    # fit the tokenizer on the documents\n",
        "    t.fit_on_texts(x)\n",
        "    return t.texts_to_sequences(x), t\n",
        "tests.test_tokenize(tokenize)\n",
        "\n",
        "# Tokenize Example output\n",
        "text_sentences = [\n",
        "    'The quick brown fox jumps over the lazy dog .',\n",
        "    'By Jove , my quick study of lexicography won a prize .',\n",
        "    'This is a short sentence .']\n",
        "text_tokenized, text_tokenizer = tokenize(text_sentences)\n",
        "print(text_tokenizer.word_index)\n",
        "\n",
        "print()\n",
        "for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n",
        "    print('Sequence {} in x'.format(sample_i + 1))\n",
        "    print('  Input:  {}'.format(sent))\n",
        "    print('  Output: {}'.format(token_sent))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MA92uMmimrO",
        "outputId": "5a40d66e-3e8d-4c87-ea75-34d019fc0fff"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'the': 1, 'quick': 2, 'a': 3, 'brown': 4, 'fox': 5, 'jumps': 6, 'over': 7, 'lazy': 8, 'dog': 9, 'by': 10, 'jove': 11, 'my': 12, 'study': 13, 'of': 14, 'lexicography': 15, 'won': 16, 'prize': 17, 'this': 18, 'is': 19, 'short': 20, 'sentence': 21}\n",
            "\n",
            "Sequence 1 in x\n",
            "  Input:  The quick brown fox jumps over the lazy dog .\n",
            "  Output: [1, 2, 4, 5, 6, 7, 1, 8, 9]\n",
            "Sequence 2 in x\n",
            "  Input:  By Jove , my quick study of lexicography won a prize .\n",
            "  Output: [10, 11, 12, 2, 13, 14, 15, 16, 3, 17]\n",
            "Sequence 3 in x\n",
            "  Input:  This is a short sentence .\n",
            "  Output: [18, 19, 3, 20, 21]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_tokenized"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEMIIiw9vBKK",
        "outputId": "a8807ad1-f343-487a-fb62-fe4b488ae280"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 2, 4, 5, 6, 7, 1, 8, 9],\n",
              " [10, 11, 12, 2, 13, 14, 15, 16, 3, 17],\n",
              " [18, 19, 3, 20, 21]]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Padding (IMPLEMENTATION)\n",
        "\n",
        "When batching the sequence of word ids together, each sequence needs to be the same length. Since sentences are dynamic in length, we can add padding to the end of the sequences to make them the same length.\n",
        "\n",
        "Make sure all the English sequences have the same length and all the French sequences have the same length by adding padding to the end of each sequence using Keras's pad_sequences function."
      ],
      "metadata": {
        "id": "x5brGG0Utgp0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pad(x, length=None):\n",
        "    \"\"\"\n",
        "    Pad x\n",
        "    :param x: List of sequences.\n",
        "    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n",
        "    :return: Padded numpy array of sequences\n",
        "    \"\"\"\n",
        "   \n",
        "    if length is None:\n",
        "        length = max([len(sentence) for sentence in x])\n",
        "    return pad_sequences(x, maxlen=length, padding='post')\n",
        "tests.test_pad(pad)\n",
        "\n",
        "# Pad Tokenized output\n",
        "test_pad = pad(text_tokenized)\n",
        "for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n",
        "    print('Sequence {} in x'.format(sample_i + 1))\n",
        "    print('  Input:  {}'.format(np.array(token_sent)))\n",
        "    print('  Output: {}'.format(pad_sent))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75bed0kqpYhI",
        "outputId": "ad3374da-415c-4910-b5d6-a72ac0e9ca8b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence 1 in x\n",
            "  Input:  [1 2 4 5 6 7 1 8 9]\n",
            "  Output: [1 2 4 5 6 7 1 8 9 0]\n",
            "Sequence 2 in x\n",
            "  Input:  [10 11 12  2 13 14 15 16  3 17]\n",
            "  Output: [10 11 12  2 13 14 15 16  3 17]\n",
            "Sequence 3 in x\n",
            "  Input:  [18 19  3 20 21]\n",
            "  Output: [18 19  3 20 21  0  0  0  0  0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocess Pipeline\n",
        "\n",
        "Your focus for this project is to build neural network architecture, so we won't ask you to create a preprocess pipeline. Instead, we've provided you with the implementation of the preprocess function."
      ],
      "metadata": {
        "id": "COSF-Qin1upi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(x, y):\n",
        "    \"\"\"\n",
        "    Preprocess x and y\n",
        "    :param x: Feature List of sentences\n",
        "    :param y: Label List of sentences\n",
        "    :return: Tuple of (Preprocessed x, Preprocessed y, x tokenizer, y tokenizer)\n",
        "    \"\"\"\n",
        "    preprocess_x, x_tk = tokenize(x)\n",
        "    preprocess_y, y_tk = tokenize(y)\n",
        "\n",
        "    preprocess_x = pad(preprocess_x)\n",
        "    preprocess_y = pad(preprocess_y)\n",
        "\n",
        "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
        "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
        "\n",
        "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
        "\n",
        "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer =\\\n",
        "    preprocess(english_sentences, french_sentences)\n",
        "    \n",
        "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
        "max_french_sequence_length = preproc_french_sentences.shape[1]\n",
        "english_vocab_size = len(english_tokenizer.word_index)\n",
        "french_vocab_size = len(french_tokenizer.word_index)\n",
        "\n",
        "print('Data Preprocessed')\n",
        "print(\"Max English sentence length:\", max_english_sequence_length)\n",
        "print(\"Max French sentence length:\", max_french_sequence_length)\n",
        "print(\"English vocabulary size:\", english_vocab_size)\n",
        "print(\"French vocabulary size:\", french_vocab_size)"
      ],
      "metadata": {
        "id": "M-JTSbPDt51b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r3ZR9byA6dlf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}